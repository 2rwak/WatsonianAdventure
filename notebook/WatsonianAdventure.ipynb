{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4801bca4",
   "metadata": {},
   "source": [
    "# WatsonianAdventure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f4977",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of this porjectis to develop a machine learning model that can detect contradiction and entailment in multilingual text. This problem has applications in various areas such as information retrieval, question answering, and text summarization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e2dc2",
   "metadata": {},
   "source": [
    "## Source of the Dataset\n",
    "The dataset was collected for the \"Contradictory, My Dear Watson\" Kaggle competition https://www.kaggle.com/competitions/contradictory-my-dear-watson/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab06ff",
   "metadata": {},
   "source": [
    "## Dataset General Info\n",
    "The dataset contains pairs of text from various sources in different languages, along with a label indicating whether the two texts are entailment, neutral, or contradiction (0, 1, or 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d7261",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "\n",
    "### Numbers:\n",
    "The dataset consists of 12,500 rows. \n",
    "There are 5,501 unique premises and 9,008 unique hypotheses in the dataset. \n",
    "\n",
    "### Class Distribution\n",
    "3,240 rows are labeled as \"contradiction\". \n",
    "5,924 rows are labeled as \"entailment\".\n",
    "3,336 rows are labeled as \"neutral\".\n",
    "\n",
    "### Missing Values\n",
    "There are no missing values in the dataset.\n",
    "\n",
    "### Statistical Summaries\n",
    "| Variable    | Mean Characters | Var Characters | Mean Words | Var Words |\n",
    "|-------------|-----------------|----------------|------------|-----------|\n",
    "| Premise     | 66.91           | 2949.08        | 12.67      | 110.98    |\n",
    "| Hypothesis | 51.51           | 1282.49        | 9.54        | 35.12      |\n",
    "\n",
    "### Sample\n",
    "| Premise                                             | Hypothesis                                    | Label         |\n",
    "|:----------------------------------------------------|:----------------------------------------------|:--------------|\n",
    "| The new rights are nice enough, but not world-shaking. | The new rights are not important.           | contradiction |\n",
    "| Electronic money is not yet a frequently used feature. | Many people are starting to use electronic money more often. | entailment    |\n",
    "| US is still a superpower.                             | The US is no longer a superpower.           | contradiction |\n",
    "| Fear the turtle.                                      | Fear the turtle.                             | neutral       |\n",
    "\n",
    "### Preprocessing Techniques\n",
    "\n",
    "1) Filtering Non English Records, which leaves us with:\n",
    "6,517 rows. \n",
    "2,304 rows are labeled as \"entailment\".\n",
    "2,060 rows are labeled as \"neutral\".\n",
    "2,152 rows are labeled as \"contradiction\". \n",
    "\n",
    "2) Removing stop words:\n",
    "Stop words are commonly used words in a language. Examples of stop words in English include \"the\", \"and\", \"a\", \"an\", \"in\", \"on\", etc. We remove them because they don't carry much meaning in the context of the text and can add noise to the analysis.\n",
    "\n",
    "3) Tokenization\n",
    "breaking down text into individual tokens or words. To represent text as a sequence of discrete symbols that can be processed by machine learning algorithms.\n",
    "\n",
    "4) Word Embeddings \n",
    "Representing text data as high-dimensional vectors. To capture the semantic relationships between words and phrases, which can improve the accuracy of your machine learning model.\n",
    "\n",
    "You can check the dataset here: https://docs.google.com/spreadsheets/d/1dK9E9EvCCtQ5EjoUfYBJT1agvHHr92vjTQR4KwdSuUM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a1723a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b455d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"/Users/2rwak/Desktop/contradictory-my-dear-watson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fda4eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter out non-English records\n",
    "df['lang_premise'] = df['premise'].apply(lambda x: detect(str(x)))\n",
    "df['lang_hypothesis'] = df['hypothesis'].apply(lambda x: detect(str(x)))\n",
    "df = df[(df['lang_premise'] == 'en') & (df['lang_hypothesis'] == 'en')]\n",
    "# drop the language columns\n",
    "df = df.drop(columns=['lang_premise', 'lang_hypothesis'])\n",
    "\n",
    "# save the filtered dataset\n",
    "df.to_csv('train_en.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339fb8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/2rwak/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/2rwak/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/2rwak/Library/Python/3.9/lib/python/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/2rwak/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /Users/2rwak/Library/Python/3.9/lib/python/site-packages (from nltk) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/2rwak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#  stop-word removal\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv('/Users/2rwak/Desktop/contradictory-my-dear-watson/train_en.csv')\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['premise'] = df['premise'].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n",
    "df['hypothesis'] = df['hypothesis'].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n",
    "\n",
    "# save the cleaned dataset\n",
    "df.to_csv('train_en_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bb4898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/2rwak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv('/Users/2rwak/Desktop/contradictory-my-dear-watson/train_en_cleaned.csv')\n",
    "\n",
    "# tokenize text\n",
    "df['premise'] = df['premise'].apply(lambda x: word_tokenize(str(x)))\n",
    "df['hypothesis'] = df['hypothesis'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# save the tokenized dataset\n",
    "df.to_csv('train_en_tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "856c9110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/2rwak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# word embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv('/Users/2rwak/Desktop/contradictory-my-dear-watson/train_en_tokenized.csv')\n",
    "\n",
    "# load the pre-trained GloVe embeddings\n",
    "# download the GloVe embeddings file from the official website at https://nlp.stanford.edu/projects/glove/.\n",
    "glove_model = KeyedVectors.load_word2vec_format('/Users/2rwak/Desktop/contradictory-my-dear-watson/glove.6B.100d.txt', binary=False, no_header=True)\n",
    "\n",
    "# convert premise and hypothesis to their vector representations\n",
    "premise_vectors = []\n",
    "for premise in df['premise']:\n",
    "    tokens = word_tokenize(premise)\n",
    "    vectors = [glove_model[word] for word in tokens if word in glove_model.key_to_index]\n",
    "    if len(vectors) > 0:\n",
    "        premise_vectors.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        premise_vectors.append(np.zeros((100,)))\n",
    "df['premise_vectors'] = premise_vectors\n",
    "\n",
    "hypothesis_vectors = []\n",
    "for hypothesis in df['hypothesis']:\n",
    "    tokens = word_tokenize(hypothesis)\n",
    "    vectors = [glove_model[word] for word in tokens if word in glove_model.key_to_index]\n",
    "    if len(vectors) > 0:\n",
    "        hypothesis_vectors.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        hypothesis_vectors.append(np.zeros((100,)))\n",
    "df['hypothesis_vectors'] = hypothesis_vectors\n",
    "\n",
    "# save the embeddings dataset\n",
    "df.to_csv('train_en_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42ec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a0872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
